{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5113756c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kastu\\\\NLP\\\\Week3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "300bf15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open('C:\\\\Users\\\\kastu\\\\NLP\\\\Week3\\CrimeAndPunishment.txt',errors=\"ignore\")\n",
    "rawtext = fin.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c79b1054",
   "metadata": {},
   "outputs": [],
   "source": [
    "crimetokens = nltk.word_tokenize(rawtext)\n",
    "text = nltk.Text(crimetokens)\n",
    "filewords = [w.lower() for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b997d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 22 of 22 matches:\n",
      "went out he was obliged to pass her kitchen , the door of which invariably stoo\n",
      "h was partitioned off from the tiny kitchen . The old woman stood facing him in\n",
      " to do -- to steal the axe from the kitchen . That the deed must be done with a\n",
      "ld only have to go quietly into the kitchen and to take the axe , and an hour l\n",
      "e . When he reached the landlady 's kitchen , the door of which was open as usu\n",
      "astasya was not only at home in the kitchen , but was occupied there , taking l\n",
      "les . Glancing , however , into the kitchen and seeing a bucket half full of wa\n",
      "was hanging to dry on a line in the kitchen and then he was a long while attent\n",
      " possible , in the dim light in the kitchen , he looked over his overcoat , his\n",
      "wnstairs and glanced in at the open kitchen door . Nastasya was standing with h\n",
      "d scarcely anyone to be seen , some kitchen garden or place of that sort . I sh\n",
      "s , brought up from the landlady 's kitchen . Raskolnikov sent in for Razumihin\n",
      "ersuade her ! ) And I 'll be in the kitchen . So here 's a chance for you to ge\n",
      "rought his fist down heavily on the kitchen stove , hurt his hand and sent one \n",
      "samovar was taken into her from the kitchen . I was not vouchsafed a personal i\n",
      " all prepared in Amalia Ivanovna 's kitchen . Two samovars were boiling , that \n",
      "tc. , and to cook the dishes in her kitchen , and Katerina Ivanovna had left it\n",
      "ould not have been allowed into the kitchen . '' Katerina Ivanovna , however , \n",
      "ld not have taken as cooks into his kitchen , and my late husband would have do\n",
      "n ! I seem to see it somewhere in a kitchen garden . It was in a kitchen garden\n",
      "e in a kitchen garden . It was in a kitchen garden , you told Zametov and after\n",
      "dosya , the cook , had not left the kitchen . And above all not a word must be \n"
     ]
    }
   ],
   "source": [
    "text.concordance('kitchen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5c13425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Produced',\n",
       " 'by',\n",
       " 'John',\n",
       " 'Bickers',\n",
       " ';',\n",
       " 'and',\n",
       " 'Dagny',\n",
       " 'CRIME',\n",
       " 'AND',\n",
       " 'PUNISHMENT',\n",
       " 'By',\n",
       " 'Fyodor',\n",
       " 'Dostoevsky',\n",
       " 'Translated',\n",
       " 'By',\n",
       " 'Constance',\n",
       " 'Garnett',\n",
       " 'TRANSLATOR',\n",
       " \"'S\",\n",
       " 'PREFACE',\n",
       " 'A',\n",
       " 'few',\n",
       " 'words',\n",
       " 'about',\n",
       " 'Dostoevsky',\n",
       " 'himself',\n",
       " 'may',\n",
       " 'help',\n",
       " 'the',\n",
       " 'English',\n",
       " 'reader',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'his',\n",
       " 'work',\n",
       " '.',\n",
       " 'Dostoevsky',\n",
       " 'was',\n",
       " 'the',\n",
       " 'son',\n",
       " 'of',\n",
       " 'a',\n",
       " 'doctor',\n",
       " '.',\n",
       " 'His',\n",
       " 'parents',\n",
       " 'were',\n",
       " 'very',\n",
       " 'hard-working',\n",
       " 'and']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crimetokens[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc9bfefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6482082e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['produc', 'by', 'john', 'bicker', ';', 'and', 'dagni', 'crime', 'and', 'punish', 'by', 'fyodor', 'dostoevski', 'translat', 'by', 'constanc', 'garnett', 'translat', \"'s\", 'prefac', 'a', 'few', 'word', 'about', 'dostoevski', 'himself', 'may', 'help', 'the', 'english', 'reader', 'to', 'understand', 'hi', 'work', '.', 'dostoevski', 'wa', 'the', 'son', 'of', 'a', 'doctor', '.', 'hi', 'parent', 'were', 'veri', 'hard-work', 'and', 'deepli', 'religi', 'peopl', ',', 'but', 'so', 'poor', 'that', 'they', 'live', 'with', 'their', 'five', 'children', 'in', 'onli', 'two', 'room', '.', 'the', 'father', 'and', 'mother', 'spent', 'their', 'even', 'in', 'read', 'aloud', 'to', 'their', 'children', ',', 'gener', 'from', 'book', 'of', 'a', 'seriou', 'charact', '.', 'though', 'alway', 'sickli', 'and', 'delic', 'dostoevski', 'came', 'out', 'third', 'in', 'the', 'final', 'examin', 'of', 'the', 'petersburg', 'school', 'of', 'engin', '.', 'there', 'he', 'had', 'alreadi', 'begun', 'hi', 'first', 'work', ',', '``', 'poor', 'folk', '.', \"''\", 'thi', 'stori', 'wa', 'publish', 'by', 'the', 'poet', 'nekrassov', 'in', 'hi', 'review', 'and', 'wa', 'receiv', 'with', 'acclam', '.', 'the', 'shi', ',', 'unknown', 'youth', 'found', 'himself', 'instantli', 'someth', 'of', 'a', 'celebr', '.', 'a', 'brilliant', 'and', 'success', 'career', 'seem', 'to', 'open', 'befor', 'him', ',', 'but', 'those', 'hope', 'were', 'soon', 'dash', '.', 'in', '1849', 'he', 'wa', 'arrest', '.', 'though', 'neither', 'by', 'tempera', 'nor', 'convict', 'a', 'revolutionist', ',', 'dostoevski', 'wa', 'one', 'of', 'a', 'littl', 'group', 'of', 'young', 'men', 'who', 'met']\n"
     ]
    }
   ],
   "source": [
    "crimePstem = [porter.stem(t) for t in crimetokens]\n",
    "print(crimePstem[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d414920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['produc', 'by', 'john', 'bick', ';', 'and', 'dagny', 'crim', 'and', 'pun', 'by', 'fyod', 'dostoevsky', 'transl', 'by', 'const', 'garnet', 'transl', \"'s\", 'prefac', 'a', 'few', 'word', 'about', 'dostoevsky', 'himself', 'may', 'help', 'the', 'engl', 'read', 'to', 'understand', 'his', 'work', '.', 'dostoevsky', 'was', 'the', 'son', 'of', 'a', 'doct', '.', 'his', 'par', 'wer', 'very', 'hard-working', 'and', 'deeply', 'religy', 'peopl', ',', 'but', 'so', 'poor', 'that', 'they', 'liv', 'with', 'their', 'fiv', 'childr', 'in', 'on', 'two', 'room', '.', 'the', 'fath', 'and', 'moth', 'spent', 'their', 'ev', 'in', 'read', 'aloud', 'to', 'their', 'childr', ',', 'gen', 'from', 'book', 'of', 'a', 'sery', 'charact', '.', 'though', 'alway', 'sick', 'and', 'del', 'dostoevsky', 'cam', 'out', 'third', 'in', 'the', 'fin', 'examin', 'of', 'the', 'petersburg', 'school', 'of', 'engin', '.', 'ther', 'he', 'had', 'already', 'begun', 'his', 'first', 'work', ',', '``', 'poor', 'folk', '.', \"''\", 'thi', 'story', 'was', 'publ', 'by', 'the', 'poet', 'nekrassov', 'in', 'his', 'review', 'and', 'was', 'receiv', 'with', 'acclam', '.', 'the', 'shy', ',', 'unknown', 'you', 'found', 'himself', 'inst', 'someth', 'of', 'a', 'celebr', '.', 'a', 'bril', 'and', 'success', 'car', 'seem', 'to', 'op', 'bef', 'him', ',', 'but', 'thos', 'hop', 'wer', 'soon', 'dash', '.', 'in', '1849', 'he', 'was', 'arrest', '.', 'though', 'neith', 'by', 'tempera', 'nor', 'convict', 'a', 'revolv', ',', 'dostoevsky', 'was', 'on', 'of', 'a', 'littl', 'group', 'of', 'young', 'men', 'who', 'met']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "crimeLstem = [lancaster.stem(t) for t in crimetokens]\n",
    "print(crimeLstem[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a46d50db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Produced', 'by', 'John', 'Bickers', ';', 'and', 'Dagny', 'CRIME', 'AND', 'PUNISHMENT', 'By', 'Fyodor', 'Dostoevsky', 'Translated', 'By', 'Constance', 'Garnett', 'TRANSLATOR', \"'S\", 'PREFACE', 'A', 'few', 'word', 'about', 'Dostoevsky', 'himself', 'may', 'help', 'the', 'English', 'reader', 'to', 'understand', 'his', 'work', '.', 'Dostoevsky', 'wa', 'the', 'son', 'of', 'a', 'doctor', '.', 'His', 'parent', 'were', 'very', 'hard-working', 'and', 'deeply', 'religious', 'people', ',', 'but', 'so', 'poor', 'that', 'they', 'lived', 'with', 'their', 'five', 'child', 'in', 'only', 'two', 'room', '.', 'The', 'father', 'and', 'mother', 'spent', 'their', 'evening', 'in', 'reading', 'aloud', 'to', 'their', 'child', ',', 'generally', 'from', 'book', 'of', 'a', 'serious', 'character', '.', 'Though', 'always', 'sickly', 'and', 'delicate', 'Dostoevsky', 'came', 'out', 'third', 'in', 'the', 'final', 'examination', 'of', 'the', 'Petersburg', 'school', 'of', 'Engineering', '.', 'There', 'he', 'had', 'already', 'begun', 'his', 'first', 'work', ',', '``', 'Poor', 'Folk', '.', \"''\", 'This', 'story', 'wa', 'published', 'by', 'the', 'poet', 'Nekrassov', 'in', 'his', 'review', 'and', 'wa', 'received', 'with', 'acclamation', '.', 'The', 'shy', ',', 'unknown', 'youth', 'found', 'himself', 'instantly', 'something', 'of', 'a', 'celebrity', '.', 'A', 'brilliant', 'and', 'successful', 'career', 'seemed', 'to', 'open', 'before', 'him', ',', 'but', 'those', 'hope', 'were', 'soon', 'dashed', '.', 'In', '1849', 'he', 'wa', 'arrested', '.', 'Though', 'neither', 'by', 'temperament', 'nor', 'conviction', 'a', 'revolutionist', ',', 'Dostoevsky', 'wa', 'one', 'of', 'a', 'little', 'group', 'of', 'young', 'men', 'who', 'met']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "crimeLemma = [wnl.lemmatize(t) for t in crimetokens]\n",
    "print(crimeLemma[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "478ecb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_filter(w):\n",
    "  # pattern to match word of non-alphabetical characters\n",
    "  pattern = re.compile('^[^a-z]+$')\n",
    "  if (pattern.match(w)):\n",
    "    return True\n",
    "  else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "402ee29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â€™s', 'a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', 'aside', 'ask', 'asking']\n"
     ]
    }
   ],
   "source": [
    "fstop = open('C:\\\\Users\\\\kastu\\\\NLP\\\\Week3\\Smart.English.stop', 'r')\n",
    "stoptext = fstop.read()\n",
    "fstop.close()\n",
    "stopwords = nltk.word_tokenize(stoptext)\n",
    "print (stopwords[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbb08924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams from file with top 50 frequencies\n",
      "(('katerina', 'ivanovna'), 0.0008468990312593629)\n",
      "(('pyotr', 'petrovitch'), 0.000683111954459203)\n",
      "(('wo', \"n't\"), 0.0004913612304004793)\n",
      "(('ca', \"n't\"), 0.00048736642364925596)\n",
      "(('pulcheria', 'alexandrovna'), 0.00048337161689803257)\n",
      "(('avdotya', 'romanovna'), 0.0004594027763906921)\n",
      "(('rodion', 'romanovitch'), 0.0003435533806052132)\n",
      "(('porfiry', 'petrovitch'), 0.00032357934684909616)\n",
      "(('marfa', 'petrovna'), 0.00030760011984420254)\n",
      "(('sofya', 'semyonovna'), 0.0002836312793368621)\n",
      "(('raskolnikov', \"'s\"), 0.00021971437131728752)\n",
      "(('amalia', 'ivanovna'), 0.0002157195645660641)\n",
      "(('young', 'man'), 0.0002077299510636173)\n",
      "(('great', 'deal'), 0.00018775591730750026)\n",
      "((\"n't\", 'understand'), 0.00013981823629281934)\n",
      "(('ilya', 'petrovitch'), 0.0001318286227903725)\n",
      "(('ivanovna', \"'s\"), 0.0001238390092879257)\n",
      "(('sonia', \"'s\"), 0.00011584939578547888)\n",
      "(('make', 'haste'), 0.00010785978228303205)\n",
      "(('good', 'heavens'), 0.00010386497553180865)\n",
      "\n",
      "Bigrams from file with top 50 mutual information scores\n",
      "(('praskovya', 'pavlovna'), 14.763517853413212)\n",
      "(('palais', 'de'), 14.34848035413437)\n",
      "(('de', 'cristal'), 14.348480354134367)\n",
      "(('explosive', 'lieutenant'), 14.248944680583456)\n",
      "(('semyon', 'zaharovitch'), 14.248944680583456)\n",
      "(('assistant', 'superintendent'), 13.91107504182707)\n",
      "(('arkady', 'ivanovitch'), 13.763517853413212)\n",
      "(('madame', 'resslich'), 13.567120640609708)\n",
      "(('afanasy', 'ivanovitch'), 13.34848035413437)\n",
      "(('nikodim', 'fomitch'), 13.34848035413437)\n",
      "(('andrey', 'semyonovitch'), 13.348480354134368)\n",
      "(('madame', 'lippevechsel'), 13.348480354134367)\n",
      "(('examining', 'lawyer'), 13.026552259247005)\n",
      "(('flushed', 'crimson'), 12.915520946858262)\n",
      "(('hay', 'market'), 12.91107504182707)\n",
      "(('chapter', 'iii'), 12.389122338631715)\n",
      "(('chapter', 'iv'), 12.389122338631715)\n",
      "(('dmitri', 'prokofitch'), 12.34848035413437)\n",
      "(('chapter', 'vi'), 12.348480354134367)\n",
      "(('canal', 'bank'), 12.322008142773177)\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(filewords)\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "print (\"Bigrams from file with top 50 frequencies\")\n",
    "for item in scored[:20]:\n",
    "        print (item)\n",
    "\n",
    "finder.apply_freq_filter(5)\n",
    "scored = finder.score_ngrams(bigram_measures.pmi)\n",
    "print (\"\\nBigrams from file with top 50 mutual information scores\")\n",
    "for item in scored[:20]:\n",
    "        print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "794baea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fstop = open('C:\\\\Users\\\\kastu\\\\NLP\\\\Week3\\processtextfile.py.txt', 'r')\n",
    "file0 = nltk.corpus.gutenberg.fileids( ) [0]\n",
    "emmatext = nltk.corpus.gutenberg.raw(file0)\n",
    "newemmatext = emmatext.replace('\\n', ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed049a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'book', 'is', 'interesting']\n",
      "['That', 'U', 'S', 'A', 'poster', 'print', 'costs', '12', '40', 'but', 'with', '10', 'off']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "### Development of regular expressions for tokenizing text\n",
    "import re\n",
    "# pattern to match words, i.e. anything with a sequence of word characters, ignores special chars\n",
    "shorttext = 'That book is interesting.'\n",
    "pword = re.compile('\\w+')\n",
    "print(re.findall(pword, shorttext))\n",
    "specialtext = 'That U.S.A. poster-print costs $12.40, but with 10% off.'\n",
    "print(re.findall(pword, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a17f8d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('That', ''), ('U', ''), ('S', ''), ('A', ''), ('poster-print', '-print'), ('costs', ''), ('12', ''), ('40', ''), ('but', ''), ('with', ''), ('10', ''), ('off', '')]\n",
      "[('end-of-line', '-line'), ('character', '')]\n"
     ]
    }
   ],
   "source": [
    "# pattern to match words with internal hyphens\n",
    "ptoken = re.compile('(\\w+(-\\w+)*)')\n",
    "print(re.findall(ptoken, specialtext))\n",
    "print(re.findall(ptoken, 'end-of-line character'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3b9b8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U', 'S', 'A', 'poster-print', 'costs', '12', '40', 'but', 'with', '10', 'off']\n",
      "['end-of-line', 'character']\n"
     ]
    }
   ],
   "source": [
    "# ignore the group of the inner parentheses \n",
    "ptoken = re.compile('(\\w+(?:-\\w+)*)')\n",
    "print(re.findall(ptoken, specialtext))\n",
    "print(re.findall(ptoken, 'end-of-line character'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b74221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U.S.A.']\n"
     ]
    }
   ],
   "source": [
    "# abbreviations like U.S.A.\n",
    "pabbrev = re.compile('((?:[A-Z]\\.)+)')\n",
    "print(re.findall(pabbrev, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6369f09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U', 'S', 'A', 'poster-print', 'costs', '12', '40', 'but', 'with', '10', 'off']\n"
     ]
    }
   ],
   "source": [
    "# combine this pattern with the words to make more general tokens\n",
    "ptoken = re.compile('(\\w+(?:-\\w+)*|(?:[A-Z]\\.)+)')\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68b0769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U.S.A.', 'poster-print', 'costs', '12', '40', 'but', 'with', '10', 'off']\n"
     ]
    }
   ],
   "source": [
    "# switch the order of the patterns to first match abbreviations and then other words\n",
    "ptoken = re.compile('((?:[A-Z]\\.)+|\\w+(?:-\\w+)*)')\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f17a1b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', 'but', 'with', '10', 'off']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# add expression for currency\n",
    "ptoken = re.compile('((?:[A-Z]\\.)+|\\w+(?:-\\w+)*|\\$?\\d+(?:\\.\\d+)?)')\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e007b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 'That', ''), ('U.S.A.', '', ''), ('', 'poster-print', ''), ('', 'costs', ''), ('', '', '$12.40'), ('', 'but', ''), ('', 'with', ''), ('', '10', ''), ('', 'off', '')]\n"
     ]
    }
   ],
   "source": [
    "# this is an equivalent regular expression except that it has extra parentheses\n",
    "ptoken = re.compile(r'''((?:[A-Z]\\.)+) # abbreviations, e.g. U.S.A.\n",
    " | (\\w+(?:-\\w+)*) # words with internal hyphens\n",
    " | (\\$?\\d+(?:\\.\\d+)?) # currency, like $12.40\n",
    " ''', re.X) # verbose flag\n",
    "print(re.findall(ptoken, specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db714523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: info, Domain:ischool.syr.edu.\n",
      "User: HelpfulHenry, Domain:syr.edu\n",
      "User: SageSue, Domain:syr.edu.\n"
     ]
    }
   ],
   "source": [
    "## More about findall()\n",
    "# using the findall() function to find 2 parts of each match\n",
    "email_text = \"For more information, send a request to info@ischool.syr.edu. Or you can directly contact our information staff at HelpfulHenry@syr.edu and SageSue@syr.edu.\"\n",
    "# re with two parentheses to match username and domain in every email address\n",
    "pemail = re.compile('([a-zA-Z]+)@([a-z.]+)')\n",
    "matches = re.findall(pemail, email_text)\n",
    "for m in matches:\n",
    " # format function puts each argument into the output string where the {} is\n",
    " email = 'User: {}, Domain:{}'.format(m[0],m[1])\n",
    " print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c604a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['That', 'book', 'is', 'interesting', '.']\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', ',', 'but', 'with', '10%', 'off', '.']\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', ',', 'but', 'with', '10', '%', 'off', '.']\n"
     ]
    }
   ],
   "source": [
    "### using NLTK's regular expression tokenizer\n",
    "# first define a multi-line string that is a regular expression\n",
    "pattern = r''' (?x) # set flag to allow verbose regexps\n",
    " (?:[A-Z]\\.)+ # abbreviations, e.g. U.S.A.\n",
    " |[A-Za-z]*\\'[t] #\" to take ' separated words as singke token'\"\n",
    " |[A-Z]+\\.*\\w+\\. # for words ending with .\n",
    " | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, $12.40, 50%\n",
    " | \\w+(?:-\\w+)* # words with internal hyphens\n",
    " | \\.\\.\\. # ellipsis\n",
    " | [][.,;”’?():-_%#’] # separate tokens\n",
    " | [A-Z]+\\.*\\w+\\. #for titles ending with . like Mr\n",
    " | [\\w\\.-]+'[\\w\\.-]+ '''\n",
    "# the nltk regular expression tokenizer compiles the re pattern, applies it to the text\n",
    "# and uses the matching groups to return a list of only the matched tokens\n",
    "print(nltk.regexp_tokenize(shorttext, pattern))\n",
    "print(nltk.regexp_tokenize(specialtext, pattern))\n",
    "# compare with built-in word tokenizer\n",
    "print(nltk.word_tokenize(specialtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3ca12b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweetpattern tweet1----- ['@natalieohayre', 'I', 'agree', '#hc09', 'needs', 'reform', 'but', 'not', 'by', 'crooked', 'politicians', 'who', 'r', 'clueless', 'about', 'healthcare', '!', '#tcot', '#fishy', 'NO', 'GOV', \"'\", 'T', 'TAKEOVER', '!']\n",
      "Tweetpattern tweet2----- ['To', 'Sen.', 'Roland', 'Burris', ':', 'Affordable', ',', 'quality', 'health', 'insurance', \"can't\", 'wait', 'http://bit.ly/j63je', '#hc09', '#IL', '#60660']\n",
      "Tweetpattern tweet3----- ['RT', '@karoli', ':', 'RT', '@Seriou', ':', '.', '@whitehouse', 'I', 'will', 'stand', 'w/', 'Obama', 'on', '#healthcare', ',', 'I', 'trust', 'him', '.', '#p2', '#tlot']\n",
      "Tweettokenizer  ['@natalieohayre', 'I', 'agree', '#hc09', 'needs', 'reform', '-', 'but', 'not', 'by', 'crooked', 'politicians', 'who', 'r', 'clueless', 'about', 'healthcare', '!', '#tcot', '#fishy', 'NO', \"GOV'T\", 'TAKEOVER', '!']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer for Twitter derived tweetmotif from the ARK, developed at CMU\n",
    "tweetPattern = r''' (?x) # set flag to allow verbose regexps\n",
    " (?:https?://|www)\\S+ # simple URLs\n",
    " | w/+ #for token 'w/'\n",
    " |[A-Za-z]*\\'[t] #\" to take ' separated words as singke token'\"\n",
    " |[A-Z]+\\.*\\w+\\. # for words ending with .\n",
    " | (?::-\\)|;-\\)) # small list of emoticons\n",
    " | &(?:amp|lt|gt|quot); # XML or HTML entity\n",
    " | \\#\\w+ # hashtags\n",
    " | @\\w+ # mentions \n",
    " | \\d+:\\d+ # timelike pattern\n",
    " | \\d+\\.\\d+ # number with a decimal\n",
    " | (?:\\d+,)+?\\d{3}(?=(?:[^,]|$)) # number with a comma\n",
    " | (?:[A-Z]\\.)+ # simple abbreviations\n",
    " | (?:--+) # multiple dashes\n",
    " | \\w+(?:-\\w+)* # words with internal hyphens or apostrophes\n",
    " | ['\\\".?!,:;/]+ # special characters\n",
    " '''\n",
    "tweet1 = \"@natalieohayre I agree #hc09 needs reform- but not by crooked politicians who r clueless about healthcare! #tcot #fishy NO GOV'T TAKEOVER!\"\n",
    "tweet2 = \"To Sen. Roland Burris: Affordable, quality health insurance can't wait http://bit.ly/j63je #hc09 #IL #60660\"\n",
    "tweet3 = \"RT @karoli: RT @Seriou: .@whitehouse I will stand w/ Obama on #healthcare, I trust him. #p2 #tlot\"\n",
    "print(\"Tweetpattern tweet1-----\",nltk.regexp_tokenize(tweet1,tweetPattern))\n",
    "print(\"Tweetpattern tweet2-----\",nltk.regexp_tokenize(tweet2,tweetPattern))\n",
    "print(\"Tweetpattern tweet3-----\",nltk.regexp_tokenize(tweet3,tweetPattern))\n",
    "# NLTK built-in tokenizer (more detailed version from TweetMotif)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "ttokenizer = TweetTokenizer()\n",
    "print(\"Tweettokenizer \" , ttokenizer.tokenize(tweet1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ee25376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Black', 'and', 'Mrs.', 'Brown', 'attended', 'the', 'lecture', 'by', 'Dr.', 'Gray', ',', 'but', 'Gov.', 'White', \"wasn't\", 'there', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sentence example for the question\n",
    "sent = \"Mr. Black and Mrs. Brown attended the lecture by Dr. Gray, but Gov. White wasn't there.\"\n",
    "print(nltk.regexp_tokenize(sent, pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "065328d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Mrs.', 'Dr.', 'Gov.', \"wasn't\"]\n"
     ]
    }
   ],
   "source": [
    "#separately showing the added lines\n",
    "x = r'''(?x)\n",
    "[A-Z]+\\.*\\w+\\.\n",
    "| [A-Za-z]*\\'[t]'''\n",
    "print(nltk.regexp_tokenize(sent, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc1801df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['Sen.', \" quality health insurance can't\"]\n",
      "['w/']\n"
     ]
    }
   ],
   "source": [
    "#separately showing the added lines for tweets\n",
    "z= r''' (?x)\n",
    "w/+\n",
    "|[A-Z a-z]*\\'[t]\n",
    "|[A-Z]+\\.*\\w+\\. '''\n",
    "\n",
    "print(nltk.regexp_tokenize(tweet1,z))\n",
    "print(nltk.regexp_tokenize(tweet2,z))\n",
    "print(nltk.regexp_tokenize(tweet3,z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b584f027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MS.', \"wasn't\", \"doesn't\", 'Mr.']\n",
      "['MS.', 'w/', \" it wasn't\", \" unlikely that she doesn't\", 'Mr.']\n"
     ]
    }
   ],
   "source": [
    "#own sample\n",
    "sample = '''MS. Dixitha Kasturi is an aspiring datascientist,She's a dog lover. \n",
    "w/ it wasn't unlikely that she doesn't like cakes. Mr. Hayd is one of her favourite'''\n",
    "print(nltk.regexp_tokenize(sample,x))\n",
    "print(nltk.regexp_tokenize(sample,z))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096bfc11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
